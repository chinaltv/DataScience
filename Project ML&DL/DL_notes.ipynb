{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "进入深度学习后，因为使用的包不同，对待模型的原理不再是像sklearn一样简单创建一个xx器（预测器、预处理器……），再使用内置的方法，而是先构建使用包方法构建模型结构，再使用相应方法迭代训练。  \n",
    "所有的代码，为可重复性，都应该加入随机种子。参数具体是random_state = ...，为节省空间，下述不再写作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "强化学习是深度学习里面非常特别的一类。很明显地，该数据科学方法引入了心理学行为主义/应用行为分析的思想，使用了“强化”、“奖励”等多个概念来对数据进行多轮次训练。如果说之前的数据科学（机器学习、批量学习）可以说还在建模的框架内，与科学研究中的“真理”推断只是稍微有些差异（稍微转向预测）；那么从强化学习开始，数据科学则走向了更严格的，纯粹为了目的服务的方向。  \n",
    "强化学习的核心思想是探索与利用（Exploration and Exploitation）。所谓的探索即是科研，即得到“真理”，建构对世界的认识，但不利于最后的结果产出（最大化收益）。商业上的ABtest其实就是随机对照试验RCT，属于科研精神的一环，而强化学习所强调的探索与利用的平衡，则是“算法精神”起作用，纯粹为了目的服务。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ϵ−贪心算法 Epsilon-Greedy Algorithm\n",
    "  \n",
    "# Define Action class\n",
    "class Actions:\n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "        self.mean = 0\n",
    "        self.N = 0\n",
    "\n",
    "    # Choose a random action\n",
    "    def choose(self): \n",
    "        return np.random.randn() + self.m\n",
    "\n",
    "    # Update the action-value estimate\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        self.mean = (1 - 1.0 / self.N) * self.mean + 1.0 / self.N * x\n",
    "\n",
    "def run_experiment(m1, m2, m3, eps, N):\n",
    "    actions = [Actions(m1), Actions(m2), Actions(m3)]\n",
    "    data = np.empty(N)\n",
    "    for i in range(N):   # epsilon greedy\n",
    "        p = np.random.random()\n",
    "        if p < eps:\n",
    "            j = np.random.choice(3)\n",
    "        else:\n",
    "            j = np.argmax([a.mean for a in actions])\n",
    "        x = actions[j].choose()\n",
    "        actions[j].update(x)\n",
    "    \n",
    "    # for the plot\n",
    "    data[i] = x\n",
    "    cumulative_average = np.cumsum(data) / (np.arange(N) + 1)\n",
    "    \n",
    "    # plot moving average ctr\n",
    "    plt.plot(cumulative_average)\n",
    "    plt.plot(np.ones(N)*m1)\n",
    "    plt.plot(np.ones(N)*m2)\n",
    "    plt.plot(np.ones(N)*m3)\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    for a in actions:\n",
    "        print(a.mean)\n",
    "    return cumulative_average\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    c_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000)\n",
    "    c_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000)\n",
    "    c_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 置信区间上界算法\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# import the dataset\n",
    "dataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n",
    "\n",
    "# Implementing UCB\n",
    "N = 10000\n",
    "d = 10\n",
    "ads_selected = []\n",
    "numbers_of_selections = [0] * d\n",
    "sums_of_rewards = [0] * d\n",
    "total_reward = 0\n",
    "for n in range(0, N):\n",
    "    ad = 0\n",
    "    max_upper_bound = 0\n",
    "    for i in range(0, d):\n",
    "        if numbers_of_selections[i] > 0:\n",
    "            average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n",
    "            delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n",
    "            upper_bound = average_reward + delta_i\n",
    "        else:\n",
    "            upper_bound = 1e400\n",
    "        if upper_bound > max_upper_bound:\n",
    "            max_upper_bound = upper_bound\n",
    "            ad = i\n",
    "    ads_selected.append(ad)\n",
    "    reward = dataset.values[n, ad]\n",
    "    numbers_of_selections[ad] = numbers_of_selections[ad] + 1\n",
    "    sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n",
    "    total_reward = total_reward + reward\n",
    "\n",
    "# Visualising the results\n",
    "plt.hist(ads_selected)\n",
    "plt.title('Histogram of ads selections')\n",
    "plt.xlabel('Ads')\n",
    "plt.ylabel('Number of times each ad was selected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汤普森采样/伯努利-汤普森采样/高斯-汤普森采样/softmax-汤普森采样\n",
    "\n",
    "def thompson_sampling(env, \n",
    "                      alpha=1,\n",
    "                      beta=0,\n",
    "                      n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int)\n",
    "    name = 'Thompson Sampling {}, {}'.format(alpha, beta)\n",
    "    for e in tqdm(range(n_episodes), \n",
    "                  desc='Episodes for: ' + name, \n",
    "                  leave=False):\n",
    "        samples = np.random.normal(\n",
    "            loc=Q, scale=alpha/(np.sqrt(N) + beta))\n",
    "        action = np.argmax(samples)\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] = Q[action] + (reward - Q[action])/N[action]\n",
    "\n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions\n",
    "\n",
    "class GaussianThompsonSocket():   # 本部分类基于他人的代码，使用高斯汤普森类方法处理它自己的机器人代码对象。后续应把该串代码整理成函数形式。\n",
    "    def __init__(self, q):                \n",
    "                \n",
    "        self.τ_0 = 0.0001  # the posterior precision\n",
    "        self.μ_0 = 1       # the posterior mean\n",
    "        \n",
    "        # pass the true reward value to the base PowerSocket             \n",
    "        super().__init__(q)         \n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\" return a value from the the posterior normal distribution \"\"\"\n",
    "        return (np.random.randn() / np.sqrt(self.τ_0)) + self.μ_0    \n",
    "                    \n",
    "    def update(self,R):\n",
    "        \"\"\" update this socket after it has returned reward value 'R' \"\"\"   \n",
    "\n",
    "        # do a standard update of the estimated mean\n",
    "        super().update(R)    \n",
    "               \n",
    "        # update the mean and precision of the posterior\n",
    "        self.μ_0 = ((self.τ_0 * self.μ_0) + (self.n * self.Q))/(self.τ_0 + self.n)        \n",
    "        self.τ_0 += 1      \n",
    "\n",
    "    def charge(self):\n",
    "        \"\"\" return a random amount of charge \"\"\"\n",
    "        # the reward is a guassian distribution with unit variance around the true value 'q'\n",
    "        value = np.random.randn() + self.q \n",
    "\n",
    "def softmax(env, \n",
    "            init_temp=float('inf'), \n",
    "            min_temp=0.0,\n",
    "            decay_ratio=0.04,\n",
    "            n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int)\n",
    "\n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int)\n",
    "    name = 'Lin SoftMax {}, {}, {}'.format(init_temp, \n",
    "                                           min_temp,\n",
    "                                           decay_ratio)\n",
    "    # can't really use infinity\n",
    "    init_temp = min(init_temp,\n",
    "                    sys.float_info.max)\n",
    "    # can't really use zero\n",
    "    min_temp = max(min_temp,\n",
    "                   np.nextafter(np.float32(0), \n",
    "                                np.float32(1)))\n",
    "    for e in tqdm(range(n_episodes),\n",
    "                  desc='Episodes for: ' + name, \n",
    "                  leave=False):\n",
    "        decay_episodes = n_episodes * decay_ratio\n",
    "        temp = 1 - e / decay_episodes\n",
    "        temp *= init_temp - min_temp\n",
    "        temp += min_temp\n",
    "        temp = np.clip(temp, min_temp, init_temp)\n",
    "\n",
    "        scaled_Q = Q / temp\n",
    "        norm_Q = scaled_Q - np.max(scaled_Q)\n",
    "        exp_Q = np.exp(norm_Q)\n",
    "        probs = exp_Q / np.sum(exp_Q)\n",
    "        assert np.isclose(probs.sum(), 1.0)\n",
    "\n",
    "        action = np.random.choice(np.arange(len(probs)), \n",
    "                                  size=1, \n",
    "                                  p=probs)[0]\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] = Q[action] + (reward - Q[action])/N[action]\n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习：多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
