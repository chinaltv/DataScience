{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "本质上python和sklearn对待模型的原理是创建一个xx器（预测器、预处理器……），再使用内置的方法去调用处理里面的东西。  \n",
    "所有的代码，为可重复性，都应该加入随机种子。参数具体是random_state = ...，为节省空间，下述不再写作。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理\n",
    "预处理的基本步骤分为三个。第一步是创建一个预处理器，随后使用fit方法使得预处理器和数据拟合/适配（计算一些数据的特有参数，例如均值、方差等），最后采用transform方法让数据转换。第二步和第三步也可以结合使用fit_transform函数来进行。基本语法如下：  \n",
    "```\n",
    "tool = function()\n",
    "tool.fit(df)  \n",
    "df = tool.transform(df)\n",
    "```\n",
    "第二步和第三步也可以改写成\n",
    "```\n",
    "df = tool.fit_transform(df)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些假数据，防报错\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "df = pd.DataFrame()\n",
    "x, y = df.iloc[:, :-1].values, df.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺失值处理\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_value = 'NaN', strategy = '...') # 缺失值处理器的api\n",
    "df = imputer.fit_transform(df) # 缺失值处理\n",
    "\n",
    "\n",
    "# 重编码\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # 有序离散变量编码，虚拟变量编码\n",
    "labelencoder = LabelEncoder()\n",
    "df = labelencoder.fit_transform(df)\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "df = onehotencoder.fit_transform(df).toarray()\n",
    "\n",
    "\n",
    "# 标准化/归一化\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "Scaler = StandardScaler()\n",
    "x, y = Scaler.fit_transform(x), Scaler.fit_transform(y)\n",
    "\n",
    "\n",
    "# 数据拆分\n",
    "train_rate = 0.8\n",
    "val_rate = 0.5\n",
    "\n",
    "## 数据打乱（这一步不一定需求） TODO 有实际数据再测试\n",
    "shuffle_index = np.random.permutation(len(y.ravel()))\n",
    "x = x[shuffle_index]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "## 数据拆分（训练集/测试集）\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0) # random_state用于抽样，必须一致以复现结果。另提供shuffle, stratify等参数控制抽样方法。\n",
    "\n",
    "## 数据拆分（训练集/验证集/测试集）\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(x, y, train_size=train_rate, test_size = 1-train_rate, random_state = 0) # 将数据分为训练集\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, train_size = val_rate/(1-train_rate), test_size = 1-val_rate/(1-train_rate), random_state = 0) # 将数据分为验证集和测试集"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有监督方法：传统统计方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS回归。因为OLS的回归器不会自动预处理，所以要记得对数据进行预处理，特别是标准化。\n",
    "from sklearn.linear_model import LinearRegression # OLS回归（只支持OLS拟合）\n",
    "lm = LinearRegression()\n",
    "lm.fit(X = X_train, y = y_train) # 拟合方程。此时lm是已经拟合的最小二乘线性回归方程。\n",
    "y_pred = lm.predict(X = X_val) # 计算预测y\n",
    "plt.scatter(X_train, y_train, color = 'red') # 数据散点\n",
    "plt.plot(X_train, lm.predict(X_train), color = 'blue') # 拟合线\n",
    "plt.title('Linear Regression')\n",
    "plt.show()\n",
    "\n",
    "# 模型进入方式的参数：机器学习用语\n",
    "## All-in(Enter), Backward Elimination, Forward Selection, Bidirectional Elimination（本质都是stepwise）, Score Comparison（信息量比较，例如AIC/BIC，穷尽所有可能的组合情况）\n",
    "## sklearn不提供stepwise reg，需要的应调用statsmodels.formula.api.OLS()\n",
    "\n",
    "# 单自变量多项式回归\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg_pre = PolynomialFeatures(degree = 2) # degree控制多项式的最大值\n",
    "X_poly = poly_reg_pre.fit_transform(X_train) # 因为本质上要对X进行修订，所以用的是预处理的类，修订单向量成多个向量（多变量）\n",
    "poly_reg = LinearRegression().fit(X_poly, y_train)\n",
    "\n",
    "# Logistics reg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X = X_train, y = y_train)\n",
    "y_pred = lr.predict(X = X_val)\n",
    "\n",
    "# 具有惩罚项的回归\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# 其他GLM回归\n",
    "from sklearn.linear_model import PoissonRegressor, GammaRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归任务：R^2等\n",
    "## https://blog.csdn.net/qq_34160248/article/details/127740194\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "def adj_r_squared(x_test, y_test, y_predict):\n",
    "    SS_R = sum((y_test-y_predict)**2)\n",
    "    SS_T = sum((y_test-np.mean(y_test))**2)\n",
    "    r_squared = 1 - (float(SS_R))/SS_T\n",
    "    adj_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)\n",
    "    return adj_r_squared\n",
    "\n",
    "r2 = r2_score(y_true = y_val, y_pred = y_pred)                                 # r2一般不用。也可以使用.score的方法调用线性回归器的r2\n",
    "rmse = sqrt(np.sum((y_val - y_pred) ** 2) / len(y_val))                        # 均方根误差会受到样本量的影响\n",
    "rmse = sqrt(mean_squared_error(y_true = y_val, y_pred = y_pred))               # 均方根误差会受到样本量的影响\n",
    "rse = np.sum((y_val - y_pred) ** 2) / np.sum((y_val - np.mean(y_val)) ** 2)    # 相对平方误差\n",
    "mae = np.sum(np.abs(y_val - y_pred) / len(y_val))\n",
    "mae = mean_absolute_error(y_true = y_val, y_pred = y_pred)\n",
    "adj_r2 = adj_r_squared(X_val, y_val, y_pred)                                   # 尽量使用调整r2\n",
    "\n",
    "\n",
    "# 回归任务：信息量\n",
    "## sklearn包不为非线性模型提供AIC/BIC/AICC/mallowscp。需要通过statsmodels.formula.api对线性模型进行拟合，再通过不同模型的.aic/.bic方法调取数据。不展开。\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "lasso = LassoLarsIC(criterion = 'aic') # or bic\n",
    "lasso.fit(X = X_train, y = y_train)\n",
    "lasso.alpha_\n",
    "\n",
    "\n",
    "# 分类任务：混淆矩阵等（包含I/II类错误），请注意，分类器的性能受到数据集平衡的影响。对于一分类数据需要使用其他方法。\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, auc, roc_auc_score, roc_curve, multilabel_confusion_matrix\n",
    "cm = confusion_matrix(y_true = y_val, y_pred = y_pred)         # 双类别混淆矩阵，多类别应使用multilable_cm\n",
    "# 该混淆矩阵在二分类的结构为（注意，这与正常的混淆矩阵样式是不同的）：\n",
    "#         test\t\n",
    "#         阴     阳\n",
    "# gs  阴  tn     fp\n",
    "#     阳  fn     tp\n",
    "accuracy = accuracy_score(y_true = y_val, y_pred = y_pred)     # 准确率\n",
    "precision = precision_score(y_true = y_val, y_pred = y_pred)   # 简单精度\n",
    "recall = recall_score(y_true = y_val, y_pred = y_pred)         # 召回率\n",
    "f1 = f1_score(y_true = y_val, y_pred = y_pred)                 # f1值\n",
    "# 以下调用方法在单组和多组分类（完全不平衡数据/异常值检测/多分类任务）时不适用。只适用于二分类数据。\n",
    "true_negative = cm[0][0]                                # 真阴性数量\n",
    "false_negative = cm[1][0]                               # 假阴性数量\n",
    "true_positive = cm[1][1]                                # 真阳性数量\n",
    "false_positive = cm[0][1]                               # 假阳性数量\n",
    "fpr = false_positive / (false_positive + true_negative) # 假阳性率，alpha，I类错误\n",
    "fnr = false_negative / (false_negative + true_positive) # 假阴性率，beta，II类错误"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化：分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类任务：绘图：混淆矩阵，ROC曲线，auc，CAP（累计准确）曲线，增益曲线，提升曲线\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc, plot_precision_recall, plot_cumulative_gain, plot_lift_curve\n",
    "\n",
    "# 混淆矩阵\n",
    "ConfusionMatrixDisplay(cm)\n",
    "ConfusionMatrixDisplay(y_val, y_pred)\n",
    "plot_confusion_matrix(y_true = y_val, y_pred = y_pred)\n",
    "\n",
    "# 受试者操作曲线及曲线下面积 ROC+auc\n",
    "## 梯形auc/ROCauc不适用于不平衡数据。如有不平衡数据，应采用精确召回曲线precision-recall curve\n",
    "y_pred_p1 = lr.predict_proba(X_val)[:,1]                                  # 应使用模型计算预测概率而非预测结果，以调用ROC AUCH法auc\n",
    "y_pred_p2 = lr.predict_proba(X_val)                                       # 不太确定预测概率需不需要使用后面的调用内容，等有具体工程时验证\n",
    "fpr, tpr, threshold = roc_curve(y_true = y_val, y_pred = y_pred)          # ROC曲线\n",
    "Auc = auc(x = fpr, y = tpr)                                               # 梯形法auc\n",
    "roc_auc = roc_auc_score(y_true = y_val, y_pred = y_pred_p1)               # ROC AUCH法auc。该方法使用y_pred_p1\n",
    "## sklearn包的ROC曲线绘图\n",
    "## 其他自定义参数：name（ROC曲线名），color（颜色）等。本质上这是一个运用plt的绘图，所运用的函数是plot函数，因此其他附加的参数都可以附加到kwargs字典-中传入。\n",
    "RocCurveDisplay.from_estimator(lr, X_test, y_test)\n",
    "RocCurveDisplay.from_predictions(y_true = y_val, y_pred = y_pred)\n",
    "## scikitplot包的ROC曲线绘图\n",
    "plot_roc(y_true = y_val,  y_probas = y_pred_p2)                           # 该方法使用y_pred_p2\n",
    "\n",
    "# 精度召回曲线\n",
    "prec, recall, threshold = precision_recall_curve(y_true = y_val, probas_pred = y_pred)\n",
    "PrecisionRecallDisplay(precision = prec, recall = recall).plot()\n",
    "plot_precision_recall(y_true = y_val, y_probas = y_pred_p2)               # 该方法使用y_pred_p2\n",
    "\n",
    "# 累计准确曲线 Cumulative Accuracy Profile, CAP。该函数未测试，并仅适用于0-1编码的二分类测量\n",
    "def CAP(y_true = y_val, y_pred = y_pred):\n",
    "    lm = [y for _, y in sorted(zip(y_pred, y_true), reverse=True)]\n",
    "    x = np.arange(0, len(y_true)+1)\n",
    "    y = np.append([0], np.cumsum(lm))\n",
    "    plt.figure(figsize = (20, 12))\n",
    "    plt.plot([0, len(y_true)], [0, np.sum(y_true)], c = 'b', linestyle = '--', label = 'Random Model')\n",
    "    plt.plot(x, y, c='r', label = 'Random Forest Classifier')\n",
    "    plt.plot([0, np.sum(y_true), len(y_true)], [0, np.sum(y_true), np.sum(y_true)],\n",
    "            c = 'grey', linewidth = 2, label = 'Perfect Model')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "CAP(y_true = y_val, y_pred = y_pred)\n",
    "\n",
    "# 增益曲线 Gain curve   不用于评估模型，只用于商业决策\n",
    "plot_cumulative_gain(y_true = y_val, y_probas = y_pred_p2)                # 该方法使用y_pred_p2\n",
    "\n",
    "# 提升曲线 Lift curve   不用于评估模型，只用于商业决策\n",
    "plot_lift_curve(y_true = y_val, y_probas = y_pred_p2)                     # 该方法使用y_pred_p2\n",
    "\n",
    "# 绘制决策边界（暂空）\n",
    "DecisionBoundaryDisplay()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 所有作图函数都应使用：\n",
    "plt.show()\n",
    "# 来展示图表"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化：回归任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "# 散点图：预测误差可视化\n",
    "PredictionErrorDisplay(y_true = y_val, y_pred = y_pred).plot()\n",
    "\n",
    "# 所有作图函数都应使用：\n",
    "plt.show()\n",
    "# 来展示图表"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证与超参搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation k-fold 单次处理。推荐使用该方法进行模型超参优化\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GroupKFold # K-fold类方法只用于划分数据，不用于计算结果。计算结果需要使用cross_val_score\n",
    "CrossValidator = KFold(n_splits = 5)\n",
    "scores = cross_val_score(estimator = lr, X = X_train, y = y_train, cv = CrossValidator)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) # 输出准确性均值和标准差。\n",
    "\n",
    "# 一个更详细的Cross-validation流程，假设x与y已经预处理过\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "def Kfold(n_splits = 5, X_train = X_train, y_train = y_train, model = model):\n",
    "    CrossValidator = KFold(n_splits = n_splits)\n",
    "    val_score = []\n",
    "    num = 0\n",
    "    for train_index, val_index in CrossValidator.split(X = X_train, y = y_train):\n",
    "        train_X, val_X = X_train(train_index), X_train(val_index)\n",
    "        train_y, val_y = y_train(train_index), y_train(val_index)\n",
    "        model.fit(train_X, train_y)\n",
    "        pred = model.predict(val_X)\n",
    "        score = precision_score(val_y, pred)\n",
    "        print(\"Fold \" + str(num + 1) + \"============> Precision:\" + str(round(score, 4)))\n",
    "        num += 1\n",
    "        val_score.append(score)\n",
    "    return val_score\n",
    "\n",
    "# Cross-validation LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C':[1, 10, 100, 1000], 'kernel':['linear']},\n",
    "              {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.5, 0.1, 0.01, 0.001, 0.0001]}\n",
    "              ]\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "best_index = grid_search.best_index_\n",
    "best_score = grid_search.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有监督方法：机器学习方法\n",
    "只列出部分参数用于自己调参，不代表调参只有这些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假参数，代表后续自己调整。\n",
    "k = 0\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(algorithm = \"auto\", leaf_size = k, metric = \"minkowski\", metric_params = None, n_jobs = k, n_neighbors= k, p = k, weight = 'uniform')\n",
    "knn.fit(X = X_train, y = y_train)\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "\n",
    "# LDA TODO\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# 树方法：CART\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "CARTclassifier = DecisionTreeClassifier(criterion = 'gini',  # 信息准则\n",
    "                                        max_depth = k,       # 最大深度\n",
    "                                        max_features = k     # 最多分割节点，一般为特征个数的sqrt  \n",
    "                                        )\n",
    "CARTclassifier.fit(X = X_train, y = y_train)\n",
    "y_pred = CARTclassifier.predict(X_val)\n",
    "\n",
    "\n",
    "# 集成：普通随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "forest = RandomForestClassifier(n_estimators = 500,           # 树个数\n",
    "                                criterion = 'gini',           # 信息准则\n",
    "                                max_depth = 7,                # 最大深度\n",
    "                                max_features = 2              # 最多分割节点，一般为特征个数的sqrt  \n",
    "                                )\n",
    "forest.fit(X = X_train, y = y_train)\n",
    "y_pred = forest.predict(X_val)\n",
    "\n",
    "\n",
    "# classifier SVM\n",
    "from sklearn.svm import LinearSVC, LinearSVR, NuSVC, NuSVR, OneClassSVM, SVC, SVR # 线性、一分类（异常值检测）支持向量机、Nu/C支持向量机（惩罚因子支持向量机）、支持向量回归\n",
    "svm = SVC(C = 1.0,           # 惩罚系数/松弛变量 slack variable/nu 边界存在错误点的比例\n",
    "          kernel = 'linear') # 线性核，其他核函数有高斯核/径向基函数（RBF），sigmoid核（Sigmoid），Poly（Poly）\n",
    "svm.fit(X = X_train, y = y_train)\n",
    "y_pred = svm.predict(X_val)\n",
    "\n",
    "\n",
    "# Naive Bayesian 朴素贝叶斯\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, GaussianNB, MultinomialNB\n",
    "## 分别适用于特征取值为伯努利、多分类、高斯（正态）和不平衡数据集。在大多数场景中，特征是正态的，因此例子选用高斯朴素贝叶斯。然而，在NLP中，多分类或是伯努利更加常见\n",
    "BayesClassifier = GaussianNB()\n",
    "BayesClassifier.fit(X = X_train, y = y_train)\n",
    "y_pred = BayesClassifier.predict(X = X_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督方法：聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans。sklearn的kmeans类默认使用了Kmeans++方法，因此不用再调整。\n",
    "## first step：如何确定k值？\n",
    "### 1. k = sqrt(n/2), n为样本量\n",
    "from math import sqrt\n",
    "k = sqrt(len(x[0]))\n",
    "### 2. 肘部法则 Elbow Method，适用于小样本。思想理解可参考EFA/PCA的碎石图\n",
    "def Elbow(k = k, x = x):\n",
    "    wcss = []\n",
    "    for i in range (1, k):\n",
    "        kmeans = KMeans(n_cluster = 1, max_iter = 300, n_init = 10, init = 'k-means++', random_state = 0)\n",
    "        kmeans.fit(x)\n",
    "        wcss.append(kmeans.inertia_)  # 利用组内平方和wcss计算k最优解\n",
    "    plt.plot(range(1, k), wcss)\n",
    "    plt.title('The Elbow Method')\n",
    "    plt.xlabel('Number of Cluster')\n",
    "    plt.ylabel('WCSS')\n",
    "    return plt.show()\n",
    "Elbow(k, x)\n",
    "### 3. Canopy算法，利用Canopy聚类（精度较低的聚类方法）得到一个初步结果，再利用这个结果进行精度较高的KMeans聚类\n",
    "from ML_utilities import Canopy, showCanopy # \n",
    "t1, t2 = 0.6, 0.4\n",
    "gc = Canopy(x)\n",
    "gc.setThreshold(t1, t2)\n",
    "canopies = gc.clustering()\n",
    "print('Get %s initial centers.' % len(canopies)) # 其实到这步就够了\n",
    "showCanopy(canopies, x, t1, t2)\n",
    "\n",
    "## second step：运行kmeans。默认使用kmeans++来确定初始质心位置。得到的label_kmeans就是聚类后标签。\n",
    "k = 3 # 输入上一步确定的k值\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = k, init = 'k-means++')\n",
    "label_kmeans = kmeans.fit_predict(x)\n",
    "\n",
    "# 学习向量量化\n",
    "from sklearn_lvq import GlvqModel\n",
    "lvq = GlvqModel()\n",
    "\n",
    "def train_weighting_vectors(learning_rate, n_epochs, initial_weighting_vectors, training_df, y_name):\n",
    "    # Select features\n",
    "    y_idx = training_df.columns.get_loc(y_name)\n",
    "    feature_names = training_df.columns[training_df.columns != y_name]\n",
    "    feature_loc = [i for i in range(len(training_df.columns)) if i != y_idx]\n",
    "    \n",
    "    best_matching_vector = initial_weighting_vectors.copy() # Make a deep copy to compare\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Learning rate impact will decrease each epoch\n",
    "        rate = learning_rate * (1 - (epoch / float(n_epochs)))\n",
    "        \n",
    "        # Iterate through each row of the training dataset\n",
    "        \n",
    "        for idx in range(len(training_df)):\n",
    "            row = training_df.iloc[[idx], feature_loc]\n",
    "            \n",
    "            # compute the Euclidean Distance against the training row and select the column with the closest distance\n",
    "            \n",
    "            bmu = row.apply(lambda row: np.sqrt(((best_matching_vector.iloc[:, feature_loc] - row)**2).sum(axis=1)), axis=1).idxmin(axis='columns').iloc[0]\n",
    "            \n",
    "            # Select the BMU vector with the closest distance\n",
    "            selected_bmu = best_matching_vector.loc[[bmu], feature_names]\n",
    "            \n",
    "            # Compute the error between the Closest BMU and the training row\n",
    "            error = (row.reset_index(drop=True) - selected_bmu.reset_index(drop=True))\n",
    "            \n",
    "            # If the BMU has the same class as the current training row, adjust the BMU vector closer to the training row through error * learning rate\n",
    "            if best_matching_vector.loc[bmu, y_name] == training_df.iloc[idx, y_idx]:\n",
    "                best_matching_vector.loc[[bmu], feature_names] = best_matching_vector.loc[bmu, feature_names].values + (error.values * rate)\n",
    "            # Otherwise, make it them ever more far apart from the current training row to make sure they are not selected in the next iteration (i.e. Euclidean Distance)\n",
    "            else:\n",
    "                best_matching_vector.loc[[bmu], feature_names] = best_matching_vector.loc[bmu, feature_names].values - (error.values * rate)\n",
    "    \n",
    "    return best_matching_vector\n",
    "\n",
    "def predict_lqr(test_df, trained_vectors_df, y_name):\n",
    "    idx_y = test_df.columns.get_loc(y_name)\n",
    "    features_iloc = [i for i in range(len(test_df.columns)) if i != idx_y]\n",
    "    filt = test_df.iloc[:, features_iloc].apply(lambda row: np.sqrt(((trained_vectors_df.iloc[:, features_iloc] - row)**2).sum(axis=1)), axis=1).idxmin(axis=1)\n",
    "    return trained_vectors_df.loc[filt, y_name].values\n",
    "\n",
    "# 高斯混合聚类\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture()\n",
    "gmm.fit(X = X_train)\n",
    "\n",
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X = X_train)\n",
    "\n",
    "# 凝聚聚类\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_cluster = k)\n",
    "assignment = agg.fit_predict(X = X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督方法：降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非负矩阵分解\n",
    "from sklearn.decomposition import NMF, MiniBatchNMF, non_negative_factorization\n",
    "nmf = NMF(n_components = k)\n",
    "x_score = nmf.fit_transform(X = X_train)\n",
    "\n",
    "# PCA，详细代码见EFA+CFA的训练文件。KernelPCA等方法的写作方式类似\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA\n",
    "\n",
    "# 因子分析不使用sklearn包。\n",
    "\n",
    "# ICA\n",
    "from sklearn.decomposition import FastICA, fastica\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "# 流形学习 TODO\n",
    "from sklearn.manifold import TSNE\n",
    "## t-SNE，用于高维数据可视化\n",
    "from matplotlib import ticker\n",
    "\n",
    "def add_2d_scatter(ax, points, points_color, title=None):\n",
    "    x, y = points.T\n",
    "    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "def plot_2d(points, points_color, title):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n",
    "    fig.suptitle(title, size=16)\n",
    "    add_2d_scatter(ax, points, points_color)\n",
    "    plt.show()\n",
    "X_embedded = TSNE(n_components = 2, learning_rate = 'auto', init = 'pca', perplexity = 20).fit_transform(X = X_train)\n",
    "plot_2d(points = X_embedded, points_color = y_train, title = \"T-distributed Stochastic \\n Neighbor Embedding\") # 这里的points color记得要改"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关联规则学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关联规则学习：先验算法\n",
    "### 相关性的体现，采用非参数方法计算。用于离散数据。一般收到的原始数据为稀疏矩阵。\n",
    "### 预处理\n",
    "path = ''\n",
    "dataset = pd.read_csv(filepath_or_buffer = path, header = None)\n",
    "transaction = []\n",
    "for i in range(0, len(dataset)):\n",
    "    transaction.append([str(dataset.values[i, j]) for j in range(0, 20)])\n",
    "from apyori import apriori # https://zhuanlan.zhihu.com/p/71538840，不对三个指标进行解释，需要可以后续观看该专栏。\n",
    "rules = apriori(transactions = transaction, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2) # 需要根据业务情况，合理设置最小支持度、最小可信度、最小提升度\n",
    "result = [list(x) for x in list(rules)] # 出来的是变量之间的三个指标。可以后续整理成相关矩阵。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f5b707d86fd9281530b9fa2dbdbe1b33232c3b651a8e052360c651d4996094"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
